% addpath(genpath('/homes/hkim/Documents/GPstuff-4.6'));
addpath(genpath('/Users/hyunjik11/Documents/GPstuff'));
% num_workers=4;
% POOL=parpool('local',num_workers);
% % Load the data
% x=h5read('/homes/hkim/GPT/PPdata.h5','/Xtrain');
% y=h5read('/homes/hkim/GPT/PPdata.h5','/ytrain');
x=h5read('/Users/hyunjik11/Documents/GPT/PPdata_full.h5','/Xtrain');
y=h5read('/Users/hyunjik11/Documents/GPT/PPdata_full.h5','/ytrain');
%% PP500 hyperparams
%x=x(1:500,:); y=y(1:500); %only use 500 pts for faster computation.
%length_scale=[2.0368 3.0397 5.7816 6.9119];
%sigma_RBF2=0.7596;
%signal_var=0.0599;
%% PPfull hyperparams
length_scale=[1.3978 0.0028 2.8966 7.5565];
sigma_RBF2=0.8333; 
signal_var=0.0195;
[n, D] = size(x);


% Now we will use the variational sparse approximation.

% First we create the GP structure. Notice here that if we do
% not explicitly set the priors for the covariance function
% parameters they are given a uniform prior.
lik = lik_gaussian('sigma2', 0.2^2);
gpcf = gpcf_sexp('lengthScale', ones(1,D), 'magnSigma2', 0.2^2);
%lik = lik_gaussian('sigma2', signal_var);
%gpcf = gpcf_sexp('lengthScale', length_scale, 'magnSigma2', sigma_RBF2);
gp=gp_set('lik',lik,'cf',gpcf); %exact gp

%X_u=datasample(x,m,1,'Replace',false); %each row specifies coordinates of an inducing point. here we randomly sample m data points
%gp_fic = gp_set('type', 'FIC', 'lik', lik, 'cf', gpcf,'X_u', X_u); %var_gp
if 1==0
[K,C]=gp_trcov(gp,x);
%tmp=C\y;
%fprintf('innerprod/2=%4.2f \n',y'*tmp); %evaluate y'*inv(K+s^2I)*y
naive_ip_means=zeros(6,1); naive_ip_stds=zeros(6,1); k=1;
for m=[10,20,40,80,160,320] %number of inducing pts.
values=zeros(10,1);
for i=1:10
idx=randsample(n,m);
K_mn=K(idx,:); K_mm=K(idx,idx);
L_mm=chol(K_mm); %L_mm'*L_mm=K_mm;
L=L'\K_mn; %L'*L=K_hat=K_mn'*(K_mm\K_mn)
K_hat=L'*L;
%K_hat=K_hat+diag(diag(K-K_hat));
%K_hat=K_hat+blockdiag(K-K_hat,m);
value=y'*((K_hat+signal_var*eye(n))\y)/2;
%L=chol(K_hat+signal_var*eye(n));
%value=sum(log(diag(L)));
values(i)=value;
end
mymean=mean(values); mystd=std(values);
naive_ip_means(k)=mymean; naive_ip_stds(k)=mystd; k=k+1;
fprintf('m=%d, mean=%4.4f, std=%4.4f \n',m,mymean,mystd)
end
end
% Next we initialize the inducing inputs and set them in GP
% structure. We have to give a prior for the inducing inputs also,
% if we want to optimize them


% -----------------------------
% --- Conduct the inference ---

% Then we can conduct the inference. We can now optimize i) only
% the parameters, ii) both the parameters and the inducing inputs,
% or iii) only the inducing inputs. Which option is used is defined
% by a string that is given to the gp_pak, gp_unpak, gp_e and gp_g
% functions. The strings for the different options are:
% 'covariance+likelihood' (i), 'covariance+likelihood+inducing' (ii),
% 'inducing' (iii).
%

% Now you can choose, if you want to optimize only parameters or
% optimize simultaneously parameters and inducing inputs. Note that
% the inducing inputs are not transformed through logarithm when
% packed

% optimize parameters and inducing inputs
%gp_var = gp_set(gp_var, 'infer_params', 'covariance+likelihood+inducing');
% optimize only parameters
%gp_var = gp_set(gp_var, 'infer_params', 'covariance+likelihood');           

opt=optimset('TolFun',1e-3,'TolX',1e-4,'Display','iter','MaxIter',1000);%,'Display','off');
% Optimize with the quasi-Newton method
gp=gp_optim(gp,x,y,'opt',opt);
%gp_var=gp_optim(gp_var,x,y,'opt',opt,'optimf',@fminscg); %can also use @fminlbfgs,@fminunc
% Set the options for the optimization
%for m=1:10
%xm=x(1:10*m,:); ym=y(1:10*m);
%fprintf('m=%d,-l=%2.4f \n',10*m,gp_e([],gp,xm,ym));
%end
[temp,nll]=gp_e([],gp,x,y);
fprintf('-l=%2.4f;',nll);
fprintf('length_scale=[');
fprintf('%s',num2str(gp.cf{1}.lengthScale));
fprintf('];sigma_RBF2=%2.4f;signal_var=%2.4f \n',gp.cf{1}.magnSigma2,gp.lik.sigma2);
%fprintf('-l=%2.4f;',gp_e([],gp_var,x,y));
%fprintf('length_scale=[');
%fprintf('%s',num2str(gp_var.cf{1}.lengthScale));
%fprintf('];sigma_RBF2=%2.4f;signal_var=%2.4f \n',gp_var.cf{1}.magnSigma2,gp_var.lik.sigma2);
%end
% delete(POOL);
% To optimize the parameters and inducing inputs sequentially uncomment the below lines
% $$$ iter = 1
% $$$ e = gp_e(w,gp_var,x,y)
% $$$ e_old = inf;
% $$$ while iter < 100 & abs(e_old-e) > 1e-3
% $$$     e_old = e;
% $$$     
% $$$     gp_var = gp_set(gp_var, 'infer_params', 'covariance+likelihood');  % optimize parameters and inducing inputs
% $$$     gp_var=gp_optim(gp_var,x,y,'opt',opt);
% $$$     gp_var = gp_set(gp_var, 'infer_params', 'inducing');  % optimize parameters and inducing inputs
% $$$     gp_var=gp_optim(gp_var,x,y,'opt',opt);
% $$$     e = gp_e(w,gp_var,x,y);
% $$$     iter = iter +1;
% $$$     [iter e]
% $$$ end
